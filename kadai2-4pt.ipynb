{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【第２回目　課題4】 PyTorchを使った深層学習 (4): ImageNet 学習済モデルの利用．DCNN特徴抽出とファインチューニング．\n",
    "\n",
    "PyTorchでは，<a href=\"https://pytorch.org/docs/stable/torchvision/models.html\">TORCHVISION MODELS</a> を使うことで，ImageNetの学習済モデルが簡単に利用できます．\n",
    "\n",
    "### <a href=\"https://pytorch.org/docs/stable/torchvision/models.html\">学習済モデル自動読み込み</a>\n",
    "torchvision.datasets と同様に，自動ダウンロード機能を備えた torchvision.modelsのモジュール群が用意されています．\n",
    "* AlexNet\n",
    "* VGG（定番のVGG．パラメータが512MBもあって巨大なのが難点．)\n",
    "* ResNet\n",
    "* SqueezeNet\n",
    "* DenseNet\n",
    "* Inception v3\n",
    "* GoogLeNet\n",
    "* ShuffleNet v2\n",
    "* MobileNet v2 (Googleのモバイル用ネットワーク．パラメータが10MB未満．）\n",
    "* ResNeXt\n",
    "* Wide ResNet\n",
    "* MNASNet (最新の自動構築されたモバイル用ネットワーク) <a href=\"https://arxiv.org/abs/1807.11626\">(Mobile Neural Archtecture Search Network)\n",
    "    \n",
    "初回実行時には，datasetsと同様に自動的にダウンロードが行われます．（ですので，Proxyが必要な環境では，事前に環境変数を設定する必要があります．)\n",
    "\n",
    "他にも，object detection, semantic segmentation, video classification のモデルが用意されています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "import os\n",
    "\n",
    "# proxyの設定．\n",
    "# keras.datasetsでは，datasetを直接ダウンロードするので，学内マシンからは通常必要．\n",
    "os.environ[\"http_proxy\"] = \"http://proxy.uec.ac.jp:8080/\"\n",
    "os.environ[\"https_proxy\"] = \"http://proxy.uec.ac.jp:8080/\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"      # \"0\":GPU0, \"1\":GPU1, \"0,1\":GPUを2つとも使用\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50 による1000種類分類\n",
    "\n",
    "まずは，ResNet50 のpretrained modelを使って，1000種類認識をしてみましょう．\n",
    "以下のコードだけで実行できます．学習済モデルも自動的にダウンロードするので簡単です．\n",
    "なお，初回実行時は，モデルのダウンロードを行うので，結果が出るまで少し時間が掛かります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ResNet50 による 1000種類分類\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1536632\r\n",
      "-rw------- 1 yanai YANAI_LAB  32342954 Jul 18  2019 densenet121-a639ec97.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB  21383481 Jul 18  2019 efficientnet-b0-08094119.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB 108857766 Jul 19  2019 inception_v3_google-1a9a5a14.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB 178728960 Dec 26 01:29 resnet101-5d3b4d8f.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB 102502400 Dec 25 19:28 resnet50-19c8e357.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB 553433881 Jul  1  2019 vgg16-397923af.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB 574673361 Jan 10 20:48 vgg19-dcbb9e9d.pth\r\n"
     ]
    }
   ],
   "source": [
    "resnet50 = models.resnet50(pretrained=True,progress=True)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "# pretrained=True とすると，学習済みポラメータも読み込まれる．\n",
    "# ~/.cache/torch/checkpoints/ の下に読み込まれます．\n",
    "# ls でダウンロードされていることを確認してみます．\n",
    "! ls -l ~/.cache/torch/checkpoints/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchのpre-trained modelは，<a href=\"https://discuss.pytorch.org/t/whats-the-range-of-the-input-value-desired-to-use-pretrained-resnet152-and-vgg19/1683\">ここ</a> にかかれているように，画像は [0, 1] で表現されるように変換したものを，さらに，平均[0.485, 0.456, 0.406], 分散[0.229, 0.224, 0.225] となるように変換して学習されています．\n",
    "ですので，[0, 255]で読み込んだ画像を\n",
    "```python\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "```\n",
    "で，変換してから，学習済モデルに渡してやる必要があります．\n",
    "Pretrained model利用時に，これを行わないと，無意味な認識結果が出力されますので，十分に注意して下さい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img=np.array(Image.open('./lion.jpg').resize((224,224)), dtype=np.float32)\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "img=img.transpose(2,0,1)  # HWC -> CHW\n",
    "img=img[np.newaxis,...]  # adding batch axis\n",
    "img=torch.from_numpy(img)\n",
    "\n",
    "resnet50.eval() \n",
    "# batch_normalization を eval modelで計算するために，model.eval()で\n",
    "# eval modeを設定する．(学習時の平均分散を利用．)　\n",
    "# train modeだと，batch内の平均分散が使われるが，\n",
    "# この場合 batch_size=1 でbatch内平均分散が計算され，正しくない結果になる．\n",
    "# batch normalization を使ったモデルで認識する場合は，evel modeへの\n",
    "# 切り替えは必須なので，注意すること．\n",
    "\n",
    "with torch.no_grad(): # 勾配計算はしないので，no_grad modeで計算\n",
    "    out=softmax(resnet50(img)).numpy()[0]\n",
    "    # numpy()で，Tensor形式から numpy形式に変換\n",
    "    # batch_size=1 で1枚だけ認識したので，1枚目の結果だけをoutに入れるために[0]がついている\n",
    "\n",
    "top5   =np.sort(out)[:-6:-1]      # 昇順にソートされるので，最後の5つが top5\n",
    "top5idx=np.argsort(out)[:-6:-1]   # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.99934548 n02129165 lion, king of beasts, Panthera leo\n",
      "[2] 0.00010747 n02125311 cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\n",
      "[3] 0.00007851 n02117135 hyena, hyaena\n",
      "[4] 0.00007462 n02128385 leopard, Panthera pardus\n",
      "[5] 0.00006126 n02130308 cheetah, chetah, Acinonyx jubatus\n"
     ]
    }
   ],
   "source": [
    "# 認識結果の top-5 の結果の表示\n",
    "SYNSET_FILE='synset_words.txt'  # ImageNet1000 種類のカテゴリ名が書かれたファイル．\n",
    "synset=open(SYNSET_FILE).read().split('\\n')\n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に TorchVision を使った方法です．こちらの方が簡単です．\n",
    "\n",
    "TorchVision を使う場合は，以下の様にnormalizeします．\n",
    "```python\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.99930012 n02129165 lion, king of beasts, Panthera leo\n",
      "[2] 0.00013924 n02125311 cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\n",
      "[3] 0.00006286 n02117135 hyena, hyaena\n",
      "[4] 0.00005809 n02422106 hartebeest\n",
      "[5] 0.00005807 n02130308 cheetah, chetah, Acinonyx jubatus\n"
     ]
    }
   ],
   "source": [
    "# 画像の変換は, TorchVisionを使うと簡単にできます．\n",
    "import torchvision.transforms as transforms\n",
    "image_size = (224, 224) \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "image_transform = transforms.Compose([transforms.Resize(image_size),transforms.ToTensor(),normalize])\n",
    "img = Image.open('lion.jpg')\n",
    "img = image_transform(img)\n",
    "img = img.unsqueeze(0)\n",
    "resnet50.eval() \n",
    "with torch.no_grad(): \n",
    "    out=softmax(resnet50(img)).numpy()[0]\n",
    "top5   =np.sort(out)[:-6:-1]   \n",
    "top5idx=np.argsort(out)[:-6:-1] \n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50 のネットワークを表示してみます．各res blockにskip connectionが入っていること(addのconnected to をたどってみましょう．)と，ネットワークの最後にGlobalAveragePooling (出力のfeature mapが 1x1 になっています), Flatten, Denseが入っていることを確認しましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ResNet50 の表示\n",
    "print(resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 を使った特徴抽出\n",
    "\n",
    "VGG16 の fc2 から 4096次元特徴ベクトルを抽出してみましょう．\n",
    "初回実行時は，モデルのダウンロードを行うので，結果が出るまで少し時間が掛かります．\n",
    "\n",
    "4096次元ベクトルは，類似画像検索や，SVMを用いた画像分類の特徴量として利用できます．なお，画像分類は後述するfine-tuningでも可能で，一般にはそちらの方が高精度ですが，学習にはGPUが必要で時間が掛かるので，CPUのみで実行可能なSVMを使った画像分類の学習も場合によっては有用です．\n",
    "\n",
    "なお，データセットは，<a href=\"http://mm.cs.uec.ac.jp/animal.zip\">動物10種各100枚</a>を使ってください．\n",
    "Jupyter の terminal を開いて，Jupyterの作業ディレクトリに展開してください．\n",
    "```\n",
    "setenv http_proxy http://proxy.uec.ac.jp:8080/\n",
    "wget http://mm.cs.uec.ac.jp/animal.zip\n",
    "unzip animal.zip\n",
    "```\n",
    "としてください．(実行セルで， !(linux コマンド) としても実行できます．)\n",
    "\n",
    "なお，IEDの場合は /usr/local/class/object/animal, CEDの場合は/ced-home/staff/yanai/media/animal に同じデータセットがありますので，フルパス名を指定すれば，ダウンロード不要です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1536632\r\n",
      "-rw------- 1 yanai YANAI_LAB  32342954 Jul 18  2019 densenet121-a639ec97.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB  21383481 Jul 18  2019 efficientnet-b0-08094119.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB 108857766 Jul 19  2019 inception_v3_google-1a9a5a14.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB 178728960 Dec 26 01:29 resnet101-5d3b4d8f.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB 102502400 Dec 25 19:28 resnet50-19c8e357.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB 553433881 Jul  1  2019 vgg16-397923af.pth\r\n",
      "-rw------- 1 yanai YANAI_LAB 574673361 Jan 10 20:48 vgg19-dcbb9e9d.pth\r\n"
     ]
    }
   ],
   "source": [
    "vgg16 = models.vgg16(pretrained=True,progress=True)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "# pretrained=True とすると，学習済みポラメータも読み込まれる．\n",
    "# ~/.cache/torch/checkpoints/ に読み込まれます．VGG16は550MBもあるので，不要になったら消去しましょう．\n",
    "# ls でダウンロードされていることを確認してみます．\n",
    "! ls -l ~/.cache/torch/checkpoints/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.99754685 n02129165 lion, king of beasts, Panthera leo\n",
      "[2] 0.00033962 n02112137 chow, chow chow\n",
      "[3] 0.00028383 n02115913 dhole, Cuon alpinus\n",
      "[4] 0.00026045 n02106030 collie\n",
      "[5] 0.00013270 n02410509 bison\n"
     ]
    }
   ],
   "source": [
    "# 念のため，認識してみます．\n",
    "# 2位以下はResNetと若干異なっていますが，1位はあっているはずです．\n",
    "image_size = (224, 224) \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "image_transform = transforms.Compose([transforms.Resize(image_size),transforms.ToTensor(),normalize])\n",
    "img = Image.open('lion.jpg')\n",
    "img = image_transform(img)\n",
    "img = img.unsqueeze(0)\n",
    "vgg16.eval() \n",
    "with torch.no_grad():\n",
    "    out=softmax(vgg16(img)).numpy()[0]\n",
    "top5   =np.sort(out)[:-6:-1]   \n",
    "top5idx=np.argsort(out)[:-6:-1] \n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (2): Flatten()\n",
      "  (3): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=4096, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "vgg16fc7 = torch.nn.Sequential(\n",
    "    vgg16.features,\n",
    "    vgg16.avgpool,\n",
    "    Flatten(),\n",
    "    *list(vgg16.classifier.children())[:-3]  # 最後の3つのlayer(relu,dropout,fc1000)を削除\n",
    ")\n",
    "# 表示してみます．fc7 (fc4096)が最終出力になっているはずです．\n",
    "print(vgg16fc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096])\n",
      "tensor([-0.4662, -2.8360, -3.2846,  0.6673, -1.7138, -4.5680,  1.4430, -1.3399,\n",
      "        -1.5306,  1.4850, -1.2114,  0.0782,  0.2708, -0.5227, -2.2403, -1.6590,\n",
      "        -1.8959, -1.2780, -1.6734, -0.1346, -3.0772, -2.9543, -2.2964,  0.0534,\n",
      "        -0.7952,  4.3078, -2.4280,  1.2097, -0.9758,  1.4676, -0.4221, -0.6301,\n",
      "        -1.3490, -0.1666, -2.0901, -1.1264, -1.8081, -2.0467, -2.2239, -3.1684,\n",
      "         0.2978, -1.4273,  0.6979, -2.1675, -1.2621,  0.3755,  4.6138,  0.1551,\n",
      "        -1.8192, -4.0548, -3.1900,  0.8393, -3.8521, -1.2662,  1.3195, -3.0540,\n",
      "        -0.9008, -2.7653, -0.1400, -2.2235, -1.8330, -0.7277,  1.0339, -1.2487,\n",
      "        -1.7718, -1.4427, -1.8328, -1.7777, -2.6309, -1.0341, -3.1651, -1.8286,\n",
      "         0.6546, -2.0965, -2.3978,  0.9357, -1.8812, -2.3502, -0.4309, -1.3155,\n",
      "        -2.1128, -2.8751, -3.5441, -1.5315, -2.1354,  0.0543, -3.0295, -2.0644,\n",
      "        -0.2059, -0.8311, -2.8010,  0.2827,  0.5275, -2.9884, -0.3949, -0.4119,\n",
      "        -0.6293, -3.3932, -2.0517,  0.7137])\n"
     ]
    }
   ],
   "source": [
    "vgg16fc7.eval()\n",
    "with torch.no_grad():\n",
    "    fc7=vgg16fc7(img)\n",
    "print(fc7.shape)     # shapeの表示\n",
    "print(fc7[0][0:100]) # fc7特徴量を最初の100次元分だけ表示してみます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に学習画像の読み込みです．DataLoaderを使う方法もありますが，ここでは一気に2クラス分の200枚読み込んで，まとめてfc7特徴を抽出してしまいましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading 0th image\n",
      "reading 100th image\n",
      "(200, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# imglist に cat100枚，dog100枚の合計200枚分のファイル名を用意します．\n",
    "# cat dog elephant fish horse lion penguin tiger whale wildcat があります．\n",
    "imglist=glob.glob('animal/lion/*.jpg')+glob.glob('animal/tiger/*.jpg')\n",
    "\n",
    "# 200枚画像をimgsに読み込みます．\n",
    "in_size=224\n",
    "imgs = np.empty((0,in_size,in_size,3), dtype=np.float32)\n",
    "\n",
    "for i,img_path in enumerate(imglist):\n",
    "    if i%100==0:\n",
    "        print(\"reading {}th image\".format(i))\n",
    "    x = np.array(Image.open(img_path).resize((in_size,in_size)), dtype=np.float32)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    imgs = np.vstack((imgs,x))\n",
    "    \n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "imgs=(imgs/255.0-mean)/std\n",
    "imgs=imgs.transpose(0,3,1,2)  # HWC -> CHW\n",
    "img=torch.from_numpy(imgs)\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 4096)\n"
     ]
    }
   ],
   "source": [
    "# 200枚処理するので，GPUを使います．\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vgg16fc7 = vgg16fc7.to(device)\n",
    "\n",
    "vgg16fc7.eval()\n",
    "with torch.no_grad():\n",
    "    fc=vgg16fc7(img.to(device)).cpu().numpy()\n",
    "    # gpuで処理した結果を cpuに戻して，numpy形式にします．\n",
    "print(fc.shape)     # shapeの表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では，抽出したfc特徴を，5-fold cross validationで学習・評価します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=100.00000%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    " \n",
    "# 分類器のオブジェクトを生成．\n",
    "# Cはソフトマージンのハイパーパラメータです．\n",
    "model_svm = LinearSVC(C=1.0)\n",
    "num = len(fc)\n",
    "label=np.append(np.ones(num//2,dtype=np.float32),(-1)*np.ones(num//2,dtype=np.float32))\n",
    "acc=[]\n",
    "\n",
    "# 5 cross validation で分類精度評価\n",
    "for f in range(5):\n",
    "    # indexの作成．5で割ってf余る数がtestのindex, そうでない数がtrainのindex.\n",
    "    train=[n for n in range(num) if n%5!=f]\n",
    "    test =[n for n in range(num) if n%5==f]\n",
    "\n",
    "# トレーニングデータで学習する\n",
    "# label は 0,1のベクトルになります．\n",
    "    model_svm.fit(fc[train], label[train])\n",
    " \n",
    "# テストデータの分類をする\n",
    "#label_predict = model.predict(fc[test])  #0/1分類\n",
    "#predict_score = model.decision_function(fc[test]) #超平面からの符号つき距離\n",
    "    acc.append(model_svm.score(fc[test],label[test])) # 分類＋accuracyの計算\n",
    "    \n",
    "    # print('accuracy={:.5%}'.format(model_svm.score(fc[test],label[test])))\n",
    "print('accuracy={:.5%}'.format(np.average(acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 のファインチューニングによる小規模データセットの学習\n",
    "\n",
    "VGG16の学習済畳み込み層を使って，小規模画像データセットの学習を行ってみましょう．\n",
    "\n",
    "データセットは同じく動物データを使いますが，ここでは10クラスのマルチクラス分類を行います．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今度は，ImageLoaderを使ってみましょう．\n",
    "\n",
    "クラスごとにサブディレクトリが別れている場合は，<a href=\"https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder\">torchvision.datasets.ImageFolder</a>を使って，簡単に dataset objectを生成できます．ラベルデータも同時に作成してくれるので，手間が省けます．\n",
    "\n",
    "次に，<a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\">torch.utils.data.Subset</a>を使って，train/testにdatasetを分割します．\n",
    "\n",
    "あとは，MNISTの時と同じで，train/testの<a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\">DataLoader</a>を生成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# まず，image_transform を定義します．ImageFolderで指定するので，先に\n",
    "# 定義しておく必要があります．\n",
    "\n",
    "# リサイズ，CHW変換，正規化 の標準的な image_transform\n",
    "image_size = (224, 224) \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "image_transform = transforms.Compose([transforms.Resize(image_size),transforms.ToTensor(),normalize])\n",
    "\n",
    "# リサイズ，CHW変換，正規化 に加えて，+-20度のランダム回転，\n",
    "# 0.8-1.2倍のランダム縦横比変換＋0.7-1.0倍のランダムクロップ，ランダム左右反転，のデータ拡張ありの image_transform\n",
    "image_transform_aug = transforms.Compose([transforms.RandomRotation(20),\n",
    "                                          transforms.RandomResizedCrop(image_size, scale=(0.7, 1.0), ratio=(0.8, 1.2)),\n",
    "                                          transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize\n",
    "                                        ])\n",
    "\n",
    "# ImageFolfer では，クラス毎にディレクトリがあって，そのクラスの画像が入っていることを想定しています．\n",
    "# animal/lion/*.jpg, animal/dog/*.jpg, animal/cat/*.jpg  ....のような感じです．\n",
    "#   animal datasetは，まさにそのようになっています．\n",
    "dataset=torchvision.datasets.ImageFolder(root=\"./animal\", transform=image_transform) \n",
    "\n",
    "# データ拡張ありの場合は，以下のを用いる． image_transform -> image_transform_aug\n",
    "augmentation=False # データ拡張しない場合は False，拡張する場合は True にする．\n",
    "\n",
    "if augmentation:\n",
    "    # データ拡張あり (学習データのみ)\n",
    "    dataset2=torchvision.datasets.ImageFolder(root=\"./animal\", transform=image_transform_aug) \n",
    "else:\n",
    "    # データ拡張なし\n",
    "    dataset2=dataset\n",
    "    \n",
    "num = len(dataset) # animal datasetの場合，1000\n",
    "\n",
    "# indexの作成．5で割り切れない数がtestのindex, 5の倍数がtrainのindex.\n",
    "# つまり，train:text=4:1=80%:20% とする．\n",
    "train_idx=[n for n in range(num) if n%5!=0]\n",
    "test_idx =[n for n in range(num) if n%5==0]\n",
    "\n",
    "trainset  = torch.utils.data.dataset.Subset(dataset2, train_idx)\n",
    "testset   = torch.utils.data.dataset.Subset(dataset, test_idx)\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は，モデル定義です．\n",
    "\n",
    "VGG16を畳み込み層だけ読み込んだモデルvgg16と，その出力を新しく追加しする全結合2層のtop_modelを用意します．\n",
    "さらに，その2つのモデルをModel によって結合します．vgg16の出力をtop_modelの入力として，それを出力とする，vgg_modelを\n",
    "定義します．\n",
    "\n",
    "結果的に，元々のVGG16の全結合層だけを新しく入れ替えて，再学習することになります．vgg_modelの元々の部分は学習済のパラメータが設定されていて，一方，新しい部分はランダム値で初期化されます．\n",
    "\n",
    "このvgg_modelを学習します．ただし，vgg_modelの前半のレイヤーは taininable = False として，学習しないようにします．\n",
    "パラメータの更新が，追加した全結合と，vggの最後の畳み込み層のみになるので，学習時間が大幅に節約できます．\n",
    "\n",
    "このように fine-tuningでは，出力に近い層だけを学習するのが一般的です．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (2): Flatten()\n",
      "  (3): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (7): ReLU(inplace=True)\n",
      "  (8): Dropout(p=0.5, inplace=False)\n",
      "  (9): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# vgg16 のconvの学習済パラメータはfinetuneしないように凍結します．\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# finetune用の model の定義．最後のlayerをcutして，nn.Linear(4096,10) を追加\n",
    "model = torch.nn.Sequential(\n",
    "    vgg16.features,\n",
    "    vgg16.avgpool,\n",
    "    Flatten(),\n",
    "    *list(vgg16.classifier.children())[:-1],  # 最後のlayer(fc1000)を削除\n",
    "    nn.Linear(4096,10)\n",
    ")\n",
    "# 表示してみます．10class分のfc出力が最終出力になっているはずです．\n",
    "print(model)\n",
    "\n",
    "# GPUに転送します．\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課題3と同様に，学習中の経過のグラフ表示用にcallback用のShowGraphクラスを用意します．（ここは中身の理解不要．）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "# callback のクラス ShowGraphを定義\n",
    "class ShowGraph:\n",
    "    def __init__(self,max_epoch):\n",
    "        # 表示エリアの設定\n",
    "        self.fig=plt.figure(figsize=(8,4))\n",
    "        self.fig1 = self.fig.add_subplot(121)\n",
    "        self.fig1.axis([0, max_epoch, 0.5, 1.0])\n",
    "        self.fig1.set_title('accuracy')\n",
    "        self.fig1.set_ylabel('accuracy')\n",
    "        self.fig1.set_xlabel('epoch')\n",
    "        self.fig2 = self.fig.add_subplot(122)\n",
    "        self.fig2.axis([0, max_epoch, 0, 5])\n",
    "        self.fig2.set_title('loss')\n",
    "        self.fig2.set_ylabel('loss')\n",
    "        self.fig2.set_xlabel('epoch')\n",
    "        self.max_epoch=max_epoch\n",
    "        self.start=time.time()\n",
    "    \n",
    "    # 学習の最初に呼び出される\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses=[]\n",
    "        self.losses_val=[]\n",
    "        self.acc=[]\n",
    "        self.acc_val=[]\n",
    "        self.n_epoch=[]\n",
    "        self.n_epoch_v=[]\n",
    "    \n",
    "    # 各epochの最後に呼び出される\n",
    "    def on_epoch_end(self, epoch, loss, acc, vloss, vacc):\n",
    "        self.n_epoch.append(epoch)\n",
    "        self.n_epoch_v.append(epoch)\n",
    "        self.acc.append(acc)\n",
    "        self.acc_val.append(vacc)     \n",
    "        self.losses.append(loss)\n",
    "        self.losses_val.append(vloss)    \n",
    "        self.test_acc=vacc\n",
    "    \n",
    "        display.clear_output(wait = True)\n",
    "        self.fig1.plot(self.n_epoch,self.acc,\"b\")\n",
    "        self.fig1.plot(self.n_epoch_v,self.acc_val,\"r\")\n",
    "        self.fig1.legend(['train', 'test'], loc='upper left')\n",
    "        self.fig2.plot(self.n_epoch,self.losses,\"b\")\n",
    "        self.fig2.plot(self.n_epoch_v,self.losses_val,\"r\")\n",
    "        self.fig2.legend(['train', 'test'], loc='upper right')\n",
    "        display.display(self.fig)\n",
    "        \n",
    "    def on_epoch_train(self, epoch, loss, acc): # validationを評価しないepochの表示用\n",
    "        self.n_epoch.append(epoch)\n",
    "        self.acc.append(acc)\n",
    "        self.losses.append(loss)\n",
    "    \n",
    "        display.clear_output(wait = True)\n",
    "        self.fig1.plot(self.n_epoch,self.acc,\"b\")\n",
    "#        self.fig1.legend(['train', 'test'], loc='upper left')\n",
    "        self.fig2.plot(self.n_epoch,self.losses,\"b\")\n",
    "#        self.fig2.legend(['train', 'test'], loc='upper right')\n",
    "        display.display(self.fig)\n",
    "              \n",
    "    # デストラクタ(オブジェクトが消滅時に実行される)  \n",
    "    # グラフが２つ表示されるのを防止．さらに最終val acc値の表示．\n",
    "    def __del__(self):\n",
    "        display.clear_output(wait = True)\n",
    "        print(\"val_acc: \",self.test_acc) \n",
    "        print('Time: ',time.time()-self.start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を行う．課題3の MNIST とほぼ一緒．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc:  0.98\n",
      "Time:  103.57957172393799\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epoch=25\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1, min_lr=0.00001)\n",
    "#optimizer = optim.Adam(model.parameters())\n",
    "show_graph=ShowGraph(num_epoch)\n",
    "show_graph.on_train_begin();\n",
    "\n",
    "def train():\n",
    "    loss=0\n",
    "    total=0\n",
    "    total0=0\n",
    "    correct=0\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        outputs = model(inputs.to(device))\n",
    "        labels=labels.to(device)\n",
    "        loss0= loss_fn(outputs, labels)\n",
    "        loss+= loss0.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        total0+=1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss0.backward()\n",
    "        optimizer.step()\n",
    "    loss=loss/total0\n",
    "    acc=correct/total \n",
    "    return loss, acc\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "         vloss=0\n",
    "         total2=0\n",
    "         total20=0\n",
    "         correct2=0\n",
    "         for (inputs, labels) in testloader:\n",
    "             outputs = model(inputs.to(device))\n",
    "             labels=labels.to(device)\n",
    "             vloss += loss_fn(outputs, labels).item()\n",
    "             _, predicted = torch.max(outputs.data, 1)\n",
    "             total2 += labels.size(0)\n",
    "             total20+=1\n",
    "             correct2 += (predicted == labels).sum().item()\n",
    "         vloss=vloss/total20\n",
    "         vacc=correct2/total2\n",
    "    return vloss, vacc   \n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    l, a = train()   \n",
    "    lv, av = validate()\n",
    "    scheduler.step(lv) # val_lossが下がらなければlrを下げる\n",
    "    show_graph.on_epoch_end(epoch,l,a,lv,av)\n",
    "            \n",
    "del show_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 課題4\n",
    "\n",
    "以下の小問の(1)-(2)を解答すること．(3)は任意（できるだけやってみる方が望ましい．）．\n",
    "\n",
    "1. ResNet50, DenseNet, MobileNetV2で，3枚以上の画像について，それぞれ1000種類認識を行うこと．\n",
    "1. 上記のデータセットで，VGG16をfine-tuningして，画像分類を行うこと．データ拡張しない場合と，する場合を比較せよ．\n",
    "1. (2)と同様に, ResNet50, DenseNet など，別のネットワークで fine-tuningして，学習時間と精度を比較せよ．\n",
    "\n",
    "(2), (3)は以下のデータセットのどれかを利用すること．UEC-Food20 のみ20種類で，あとは10種類です．すべて1クラス100枚ずつ入っています．  \n",
    "（自分で用意可能な人は，自分で用意したものを利用してもよい．各カテゴリ100枚10クラス以上用意せよ．）\n",
    "\n",
    " * UEC-Food20 http://mm.cs.uec.ac.jp/uecfood20.zip\n",
    " * UEC-Food10 http://mm.cs.uec.ac.jp/uecfood10.zip (上記の10種類版．メモリが足りない場合にどうぞ．)\n",
    " * FlickrMaterialDatabase(FMD) http://mm.cs.uec.ac.jp/material10.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "python3_anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
